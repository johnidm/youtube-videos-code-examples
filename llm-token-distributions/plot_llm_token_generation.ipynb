{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590a2292-4009-45c2-9d47-5d0d040b5461",
   "metadata": {},
   "source": [
    "# Plot LLM Token Generation\n",
    "\n",
    "Based on - https://learn.deeplearning.ai/courses/getting-structured-llm-output/lesson/zv9cy/structured-generation-with-outlines\n",
    "\n",
    "### Outlines\n",
    "\n",
    "https://github.com/dottxt-ai/outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90d730d-847c-4876-b55e-b7733d766b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f426a3-f6da-4968-af8d-52c8a3c009cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    python-dotenv==1.0.0 \\\n",
    "    openai==1.66.3 \\\n",
    "    pandas==2.2.3 \\\n",
    "    instructor==1.7.4 \\\n",
    "    outlines==0.2.1 \\\n",
    "    transformers==4.49.0 \\\n",
    "    sentencepiece==0.2.0 \\\n",
    "    datasets==3.3.2 \\\n",
    "    numpy \\\n",
    "    matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44200d5b-a946-4934-a7c6-7eae8378aa25",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff440ee7-41fa-4e7b-92e7-d953b7540b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A simple logit processor that tracks probabilities for both structured and unstructured generation.\n",
    "\n",
    "For each token generated, we store:\n",
    "- The raw logits the model would assign naturally\n",
    "- The filtered logits after applying structural constraints\n",
    "- A mapping from vocabulary indices to token strings\n",
    "\"\"\"\n",
    "from typing import TYPE_CHECKING, Optional, Union, List, Literal, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from outlines.processors.base_logits_processor import OutlinesLogitsProcessor, Array\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from outlines.generate import Generator\n",
    "\n",
    "# Try importing pandas, but don't fail if not available\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PANDAS_AVAILABLE = False\n",
    "    pd = Any  # For type hints when pandas is not available\n",
    "\n",
    "class LogitTrackingProcessor(OutlinesLogitsProcessor):\n",
    "    \"\"\"Tracks logits for both structured and unstructured token generation.\n",
    "    \n",
    "    For each position in the sequence, stores:\n",
    "    - unstructured_logits: Raw logits from the model\n",
    "    - structured_logits: Logits after applying constraints\n",
    "    - vocab_tokens: Mapping from vocab indices to token strings\n",
    "    \n",
    "    Each logit matrix has:\n",
    "    - Columns: One for each position in the generated sequence\n",
    "    - Rows: One for each token in the vocabulary\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    processor : Optional[OutlinesLogitsProcessor]\n",
    "        The processor that applies structural constraints\n",
    "    unstructured_logits : List[NDArray]\n",
    "        Raw logits from the model for each position\n",
    "    structured_logits : List[NDArray]\n",
    "        Logits after applying constraints for each position\n",
    "    vocab_tokens : Optional[List[str]]\n",
    "        Mapping from vocabulary indices to token strings\n",
    "    chosen_tokens : List[int]\n",
    "        Track actual chosen token IDs during generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor=None):\n",
    "        \"\"\"Initialize the tracking processor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        processor : Optional[OutlinesLogitsProcessor]\n",
    "            The processor that applies structural constraints.\n",
    "            If None, only tracks raw logits.\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.unstructured_logits = []  # List of logit arrays, one per position\n",
    "        self.structured_logits = []    # List of logit arrays, one per position\n",
    "        self.vocab_tokens = None      # Will store the vocabulary mapping\n",
    "        self.chosen_tokens = []       # Track actual chosen tokens during generation\n",
    "        \n",
    "    def process_logits(self, input_ids: Array, logits: Array) -> Array:\n",
    "        \"\"\"Process logits and store them.\n",
    "        \n",
    "        This method:\n",
    "        1. Stores the raw logits from the model\n",
    "        2. Applies any structural constraints if a processor exists\n",
    "        3. Stores the constrained logits\n",
    "        4. Tracks the chosen token ID\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : Array\n",
    "            The input token ids for each sequence in the batch\n",
    "        logits : Array\n",
    "            The original logits to process, shape (batch_size, vocab_size)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Array\n",
    "            The processed logits, shape (batch_size, vocab_size)\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        - For unconstrained generation (no processor), structured = unstructured\n",
    "        - Token IDs are tracked from input_ids to ensure we capture the actual choices\n",
    "        \"\"\"\n",
    "        # Always store the raw logits as unstructured\n",
    "        self.unstructured_logits.append(logits[0].detach().cpu().numpy().copy())\n",
    "        \n",
    "        # Store the actual chosen token ID if available\n",
    "        if len(input_ids[0]) > 0:\n",
    "            self.chosen_tokens.append(input_ids[0][-1].item())\n",
    "        \n",
    "        # Apply structural constraints if we have a processor\n",
    "        if self.processor is not None:\n",
    "            processed = self.processor.process_logits(input_ids, logits)\n",
    "            self.structured_logits.append(processed[0].detach().cpu().numpy().copy())\n",
    "            return processed\n",
    "            \n",
    "        # For unconstrained generation, structured = unstructured\n",
    "        self.structured_logits.append(logits[0].detach().cpu().numpy().copy())\n",
    "        return logits\n",
    "            \n",
    "    def get_probabilities(self, as_matrix: bool = False) -> Dict[str, Union[List[NDArray], NDArray]]:\n",
    "        \"\"\"Get probability distributions computed from stored logits.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        as_matrix : bool\n",
    "            If True, convert probability lists to matrices.\n",
    "            Each matrix will have shape (vocab_size, n_positions)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Union[List[NDArray], NDArray]]\n",
    "            Contains:\n",
    "            - unstructured: Raw probability distributions\n",
    "            - structured: Probability distributions after constraints\n",
    "            Each can be either a list of arrays or a single matrix\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        unstructured_probs = [\n",
    "            torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "            for logits in self.unstructured_logits\n",
    "        ]\n",
    "        structured_probs = [\n",
    "            torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "            for logits in self.structured_logits\n",
    "        ]\n",
    "        \n",
    "        if as_matrix:\n",
    "            # Stack arrays into matrices\n",
    "            unstructured = np.column_stack(unstructured_probs)\n",
    "            structured = np.column_stack(structured_probs)\n",
    "        else:\n",
    "            # Return as lists\n",
    "            unstructured = unstructured_probs\n",
    "            structured = structured_probs\n",
    "            \n",
    "        return {\n",
    "            'unstructured': unstructured,\n",
    "            'structured': structured\n",
    "        }\n",
    "\n",
    "    def get_logits(self, as_matrix: bool = False) -> Dict[str, Union[List[NDArray], NDArray]]:\n",
    "        \"\"\"Get the stored logit values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        as_matrix : bool\n",
    "            If True, convert logit lists to matrices.\n",
    "            Each matrix will have shape (vocab_size, n_positions)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Union[List[NDArray], NDArray]]\n",
    "            Contains:\n",
    "            - unstructured: Raw logit values\n",
    "            - structured: Logit values after constraints\n",
    "            Each can be either a list of arrays or a single matrix\n",
    "        \"\"\"\n",
    "        if as_matrix:\n",
    "            unstructured = np.column_stack(self.unstructured_logits)\n",
    "            structured = np.column_stack(self.structured_logits)\n",
    "        else:\n",
    "            unstructured = self.unstructured_logits\n",
    "            structured = self.structured_logits\n",
    "            \n",
    "        return {\n",
    "            'unstructured': unstructured,\n",
    "            'structured': structured\n",
    "        }\n",
    "        \n",
    "    def get_top_tokens(\n",
    "        self,\n",
    "        k: int = 10,\n",
    "        positions: Optional[Union[int, List[int]]] = None,\n",
    "        include_logits: bool = True\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get the top k tokens at specified positions with their probabilities and logits.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int, optional\n",
    "            Number of top tokens to return, by default 10\n",
    "        positions : Union[int, List[int]], optional\n",
    "            Position(s) to analyze. Can be a single position or list of positions.\n",
    "            By default analyzes all positions.\n",
    "        include_logits : bool, optional\n",
    "            Whether to include raw logit values in addition to probabilities\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[Dict[str, Any]]\n",
    "            List of dictionaries, one per position, containing:\n",
    "            - position: Position in sequence\n",
    "            - text_so_far: Text generated up to this position\n",
    "            - tokens: List of top k token dictionaries, each containing:\n",
    "                - token: The token string\n",
    "                - natural_prob: Unconstrained probability\n",
    "                - constrained_prob: Probability after constraints\n",
    "                - natural_logit: Raw logit value (if include_logits=True)\n",
    "                - constrained_logit: Constrained logit value (if include_logits=True)\n",
    "                - is_chosen: Whether this token was actually chosen\n",
    "        \"\"\"\n",
    "        # Convert single position to list\n",
    "        if positions is None:\n",
    "            positions = list(range(len(self.structured_logits)))\n",
    "        elif isinstance(positions, int):\n",
    "            positions = [positions]\n",
    "            \n",
    "        # Get probabilities and logits\n",
    "        probs = self.get_probabilities()\n",
    "        logits = self.get_logits() if include_logits else None\n",
    "        \n",
    "        # Get vocab mapping\n",
    "        vocab = self.get_vocab_mapping()\n",
    "        \n",
    "        results = []\n",
    "        for pos in positions:\n",
    "            if pos >= len(self.unstructured_logits):\n",
    "                continue\n",
    "                \n",
    "            # Get text generated so far\n",
    "            text_so_far = self.sequence(pos)\n",
    "            \n",
    "            # Get values for this position\n",
    "            u_probs = probs['unstructured'][pos]\n",
    "            s_probs = probs['structured'][pos]\n",
    "            \n",
    "            if include_logits:\n",
    "                u_logits = logits['unstructured'][pos]\n",
    "                s_logits = logits['structured'][pos]\n",
    "            \n",
    "            # Get top k indices by maximum probability\n",
    "            top_indices = np.argsort(np.maximum(u_probs, s_probs))[-k:][::-1]\n",
    "            \n",
    "            # Get the actual next token for comparison\n",
    "            next_token = self.sequence(pos + 1)[len(text_so_far):] if pos < len(self.structured_logits)-1 else \"\"\n",
    "            \n",
    "            # Build token info list\n",
    "            tokens = []\n",
    "            for idx in top_indices:\n",
    "                token = vocab[idx]\n",
    "                token_info = {\n",
    "                    'token': token,\n",
    "                    'natural_prob': float(u_probs[idx]),\n",
    "                    'constrained_prob': float(s_probs[idx]),\n",
    "                    'is_chosen': token == next_token\n",
    "                }\n",
    "                \n",
    "                if include_logits:\n",
    "                    token_info.update({\n",
    "                        'natural_logit': float(u_logits[idx]),\n",
    "                        'constrained_logit': float(s_logits[idx])\n",
    "                    })\n",
    "                    \n",
    "                tokens.append(token_info)\n",
    "            \n",
    "            results.append({\n",
    "                'position': pos,\n",
    "                'text_so_far': text_so_far,\n",
    "                'tokens': tokens\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def get_vocab_mapping(self) -> List[str]:\n",
    "        \"\"\"Get the mapping from vocabulary indices to token strings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of token strings, where index matches vocabulary index\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        AttributeError\n",
    "            If no tokenizer is available\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'tokenizer'):\n",
    "            raise AttributeError(\"No tokenizer available for mapping tokens\")\n",
    "            \n",
    "        if self.vocab_tokens is None:\n",
    "            # Create the mapping if we haven't yet\n",
    "            self.vocab_tokens = [\n",
    "                self.processor.tokenizer.decode([i])[0]\n",
    "                for i in range(len(self.unstructured_logits[0]))\n",
    "            ]\n",
    "            \n",
    "        return self.vocab_tokens\n",
    "        \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all stored logits.\"\"\"\n",
    "        self.unstructured_logits = []\n",
    "        self.structured_logits = []\n",
    "        self.chosen_tokens = []\n",
    "\n",
    "    def to_dataframe(\n",
    "        self,\n",
    "        show: Literal[\"probs\", \"logits\"] = \"probs\",\n",
    "        top_k: Optional[int] = None,\n",
    "        min_value: Optional[float] = None\n",
    "    ) -> \"pd.DataFrame\":\n",
    "        \"\"\"Convert tracking data to a pandas DataFrame for analysis.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        show : Literal[\"probs\", \"logits\"], optional\n",
    "            Whether to show probabilities or logit values, by default \"probs\"\n",
    "        top_k : Optional[int], optional\n",
    "            If provided, only include the top k tokens at each position\n",
    "            (based on maximum of structured/unstructured values)\n",
    "        min_value : Optional[float], optional\n",
    "            If provided, only include tokens with values >= min_value\n",
    "            in either structured or unstructured distribution\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with columns:\n",
    "            - position: Token position in sequence\n",
    "            - token: String representation of token\n",
    "            - natural: Raw model values (probs/logits)\n",
    "            - constrained: Values after constraints\n",
    "            \n",
    "        Examples\n",
    "        --------\n",
    "        >>> # Get probability data for top 10 tokens\n",
    "        >>> df = processor.to_dataframe(show=\"probs\", top_k=10)\n",
    "        >>> df.sort_values(\"natural\", ascending=False).head()\n",
    "        >>>\n",
    "        >>> # Get logit data above threshold\n",
    "        >>> df = processor.to_dataframe(show=\"logits\", min_value=-5)\n",
    "        >>> df.query(\"position == 0\").nlargest(5, \"natural\")\n",
    "        >>>\n",
    "        >>> # Get all tokens with probability > 1%\n",
    "        >>> df = processor.to_dataframe(show=\"probs\", min_value=0.01)\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ImportError\n",
    "            If pandas is not installed\n",
    "        \"\"\"\n",
    "        if not PANDAS_AVAILABLE:\n",
    "            raise ImportError(\n",
    "                \"pandas is required for DataFrame support. \"\n",
    "                \"Please install it with: pip install pandas\"\n",
    "            )\n",
    "            \n",
    "        # Get values based on show parameter\n",
    "        if show == \"probs\":\n",
    "            values = self.get_probabilities()\n",
    "        else:\n",
    "            values = self.get_logits()\n",
    "            \n",
    "        # Get vocab mapping\n",
    "        vocab = self.get_vocab_mapping()\n",
    "        \n",
    "        # Create lists to store data\n",
    "        rows = []\n",
    "        \n",
    "        # Process each position\n",
    "        for pos in range(len(self.unstructured_logits)):\n",
    "            u_vals = values['unstructured'][pos]\n",
    "            s_vals = values['structured'][pos]\n",
    "            \n",
    "            # Get indices to include based on filters\n",
    "            if top_k is not None or min_value is not None:\n",
    "                # Get maximum value between structured/unstructured for sorting\n",
    "                max_vals = np.maximum(u_vals, s_vals)\n",
    "                \n",
    "                if top_k is not None and min_value is not None:\n",
    "                    # Both filters: get top k among values >= min_value\n",
    "                    valid_indices = np.where(max_vals >= min_value)[0]\n",
    "                    if len(valid_indices) > top_k:\n",
    "                        valid_indices = valid_indices[np.argsort(max_vals[valid_indices])[-top_k:]]\n",
    "                elif top_k is not None:\n",
    "                    # Just top k: get indices of k largest values\n",
    "                    valid_indices = np.argsort(max_vals)[-top_k:]\n",
    "                else:  # min_value is not None\n",
    "                    # Just threshold: get indices of all values >= min_value\n",
    "                    valid_indices = np.where(max_vals >= min_value)[0]\n",
    "            else:\n",
    "                # No filters: include all tokens\n",
    "                valid_indices = range(len(vocab))\n",
    "            \n",
    "            # Add rows for valid indices\n",
    "            for idx in valid_indices:\n",
    "                rows.append({\n",
    "                    'position': pos,\n",
    "                    'token': vocab[idx],\n",
    "                    'natural': u_vals[idx],\n",
    "                    'constrained': s_vals[idx]\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def sequence(self, pos: Optional[int] = None) -> str:\n",
    "        \"\"\"Get the sequence of tokens generated up to a position.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pos : Optional[int], optional\n",
    "            Position to reconstruct up to (exclusive).\n",
    "            If None, returns the entire sequence.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The concatenated string of chosen tokens\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        AttributeError\n",
    "            If no tokenizer is available for decoding\n",
    "        \"\"\"\n",
    "        if not self.chosen_tokens:\n",
    "            return \"\"\n",
    "            \n",
    "        if not hasattr(self, 'tokenizer'):\n",
    "            raise AttributeError(\"No tokenizer available for decoding sequence\")\n",
    "            \n",
    "        # Get the tokenizer\n",
    "        if hasattr(self.processor, 'tokenizer'):\n",
    "            tokenizer = self.processor.tokenizer\n",
    "        else:\n",
    "            tokenizer = self.tokenizer\n",
    "            \n",
    "        # Get tokens up to the specified position\n",
    "        end_pos = len(self.chosen_tokens) if pos is None else pos\n",
    "        tokens_to_decode = self.chosen_tokens[:end_pos]\n",
    "        \n",
    "        # Decode the sequence\n",
    "        return \"\".join(tokenizer.decode(tokens_to_decode))\n",
    "\n",
    "\n",
    "def track_logits(generator: \"Generator\") -> \"Generator\":\n",
    "    \"\"\"Add probability tracking to any generator.\n",
    "    \n",
    "    This is a convenience function that wraps a generator's logits processor\n",
    "    with a LogitTrackingProcessor, enabling analysis of token probabilities\n",
    "    during generation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    generator : Generator\n",
    "        The generator to add tracking to\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Generator\n",
    "        The same generator with tracking enabled\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Track probabilities for unconstrained text generation\n",
    "    >>> generator = generate.text(model)\n",
    "    >>> generator = track_logits(generator)\n",
    "    >>>\n",
    "    >>> # Track probabilities for JSON generation\n",
    "    >>> generator = generate.json(model, schema)\n",
    "    >>> generator = track_logits(generator)\n",
    "    \"\"\"\n",
    "    # If there's no logits_processor, throw an error. Logit tracking\n",
    "    # is currently only supported for structured generators.\n",
    "    if generator.logits_processor is None:\n",
    "        raise ValueError(\"Logit tracking is not supported for this generator\")\n",
    "\n",
    "    # Create tracking processor, wrapping any existing processor\n",
    "    tracking = LogitTrackingProcessor(generator.logits_processor)\n",
    "\n",
    "    # Add tokenizer for token mapping\n",
    "    if hasattr(generator.logits_processor, 'tokenizer'):\n",
    "        tracking.tokenizer = generator.logits_processor.tokenizer\n",
    "    \n",
    "    # Set as the generator's processor\n",
    "    generator.logits_processor = tracking\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# This function applies a simple chat template to the prompt\n",
    "def template(model, prompt: str, system_prompt: str = \"You are a helpful assistant, responding in JSON.\") -> str:\n",
    "    return model.tokenizer.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_bos=True,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_token_distributions(tracking_processor, k=10, positions=None, show_print=False) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Print token probability distributions before and after applying constraints.\n",
    "\n",
    "    Shows:\n",
    "    - Unconstrained probabilities (what tokens the model would naturally choose)\n",
    "    - Constrained probabilities (what tokens are allowed by structural constraints)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tracking_processor : LogitTrackingProcessor\n",
    "        The processor containing tracked probabilities\n",
    "    k : int, optional\n",
    "        Number of top tokens to show, by default 10\n",
    "    positions : List[int], optional\n",
    "        Which positions to display. If None, displays all positions.\n",
    "    \"\"\"\n",
    "    probs = tracking_processor.get_probabilities(as_matrix=True)\n",
    "    vocab = tracking_processor.get_vocab_mapping()\n",
    "\n",
    "    if positions is None:\n",
    "        positions = list(range(probs['unstructured'].shape[1]))\n",
    "\n",
    "    tokens = []\n",
    "    \n",
    "    for pos in positions:\n",
    "        unstructured = probs['unstructured'][:, pos]\n",
    "        structured = probs['structured'][:, pos]\n",
    "\n",
    "        # Get top k tokens by maximum probability\n",
    "        max_probs = np.maximum(unstructured, structured)\n",
    "        top_indices = np.argsort(max_probs)[-k:][::-1]\n",
    "\n",
    "        if show_print:\n",
    "            print(f\"\\n=== Position {pos} ===\")\n",
    "            print(f\"{'Token':<20} {'Unconstrained':>15} {'Constrained':>15}\")\n",
    "            print(\"-\" * 52)\n",
    "\n",
    "        for idx in top_indices:\n",
    "            token = vocab[idx]\n",
    "            u_prob = unstructured[idx]\n",
    "            s_prob = structured[idx]\n",
    "\n",
    "            if show_print:\n",
    "                print(f\"{token:<20} {u_prob:>14.2%} {s_prob:>14.2%}\")\n",
    "\n",
    "            tokens.append( (token, u_prob, s_prob,) )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def plot_token_distributions(tracking_processor, k=10, positions=None, prefix=\"\"):\n",
    "    \"\"\"Plot token probability distributions before and after applying constraints.\n",
    "    \n",
    "    Creates a horizontal bar chart showing:\n",
    "    - Blue bars: What tokens the model would naturally choose\n",
    "    - Orange bars: What tokens are allowed by structural constraints\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tracking_processor : LogitTrackingProcessor\n",
    "        The processor containing tracked probabilities\n",
    "    k : int, optional\n",
    "        Number of top tokens to show in each plot, by default 10\n",
    "    positions : List[int], optional\n",
    "        Which positions to plot. If None, plots all positions.\n",
    "    prefix : str, optional\n",
    "        Prefix for the output filename\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - Bar height indicates probability (how likely the model thinks each token is)\n",
    "    - Tokens are sorted by maximum probability across both distributions\n",
    "    - Only probabilities > 1% show their exact values\n",
    "    - Grid lines help compare probabilities between tokens\n",
    "    \"\"\"\n",
    "    # Get probability matrices and vocab mapping\n",
    "    probs = tracking_processor.get_probabilities(as_matrix=True)\n",
    "    vocab = tracking_processor.get_vocab_mapping()\n",
    "    \n",
    "    # Determine positions to plot\n",
    "    if positions is None:\n",
    "        positions = list(range(probs['unstructured'].shape[1]))\n",
    "    n_positions = len(positions)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(1, n_positions)\n",
    "    if n_positions == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, pos in enumerate(positions):\n",
    "        # Get probabilities for this position\n",
    "        unstructured = probs['unstructured'][:, pos]\n",
    "        structured = probs['structured'][:, pos]\n",
    "        \n",
    "        # Get top k tokens by maximum probability\n",
    "        top_indices = np.argsort(np.maximum(unstructured, structured))[-k:]\n",
    "        \n",
    "        # Create bar positions\n",
    "        y = np.arange(len(top_indices))\n",
    "        height = 0.35\n",
    "        \n",
    "        # Plot bars\n",
    "        axes[idx].barh(y - height/2, unstructured[top_indices], height, \n",
    "                      label='Unconstrained', alpha=0.7, color='skyblue')\n",
    "        axes[idx].barh(y + height/2, structured[top_indices], height,\n",
    "                      label='Constrained', alpha=0.7, color='orange')\n",
    "        \n",
    "        # Customize plot\n",
    "        axes[idx].set_title('Next token probability')\n",
    "        axes[idx].set_yticks(y)\n",
    "        axes[idx].set_yticklabels([vocab[i] for i in top_indices])\n",
    "        axes[idx].set_xlabel('Probability')\n",
    "        axes[idx].tick_params(axis='both', labelsize=16)  # This changes tick label sizes\n",
    "        \n",
    "        # Add legend\n",
    "        axes[idx].legend(loc='lower right', bbox_to_anchor=(1, 1.1))\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add probability values\n",
    "        for i, (v1, v2) in enumerate(zip(unstructured[top_indices], structured[top_indices])):\n",
    "            if v1 > 0.01:  # Only show probabilities > 1%\n",
    "                axes[idx].text(v1 + 0.01, i - height/2, f'{v1:.1%}', va='center')\n",
    "            if v2 > 0.01:\n",
    "                axes[idx].text(v2 + 0.01, i + height/2, f'{v2:.1%}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{prefix}token_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_heatmap(tracking_processor, k=50, positions=None, prefix=\"\", show_both=True, kind=\"logits\", show_tokens=True):\n",
    "    \"\"\"Plot a heatmap of token probabilities across sequence positions.\n",
    "    \n",
    "    Creates a heatmap visualization showing how token probabilities evolve\n",
    "    across different positions in the sequence. Optionally shows both\n",
    "    natural and constrained probabilities side by side.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tracking_processor : LogitTrackingProcessor\n",
    "        The processor containing tracked probabilities\n",
    "    k : int, optional\n",
    "        Number of top tokens to include in the heatmap, by default 50\n",
    "    positions : List[int], optional\n",
    "        Which positions to plot. If None, plots all positions.\n",
    "    prefix : str, optional\n",
    "        Prefix for the output filename\n",
    "    show_both : bool, optional\n",
    "        If True, shows both natural and constrained probabilities side by side.\n",
    "        If False, only shows natural probabilities.\n",
    "    kind : str, optional\n",
    "        Whether to plot logits or probabilities, by default \"logits\"\n",
    "    show_tokens : bool, optional\n",
    "        Whether to show the token strings on the y-axis, by default True\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - Brighter colors indicate higher probabilities\n",
    "    - Y-axis shows token strings\n",
    "    - X-axis shows position in sequence\n",
    "    - Near-zero probabilities are masked out (shown in gray)\n",
    "    - For constrained generation, blocked tokens appear masked\n",
    "    \"\"\"\n",
    "    # Get probability matrices and vocab mapping\n",
    "    if kind == \"logits\":\n",
    "        things = tracking_processor.get_logits(as_matrix=True)\n",
    "        # For logits, mask out very negative values\n",
    "        threshold = -1e9  # Logits below this are effectively zero probability\n",
    "    else:\n",
    "        things = tracking_processor.get_probabilities(as_matrix=True)\n",
    "        # For probabilities, mask out near-zero values\n",
    "        threshold = 0.001  # Probabilities below 0.1% are masked\n",
    "    \n",
    "    vocab = tracking_processor.get_vocab_mapping()\n",
    "    \n",
    "    # Determine positions to plot\n",
    "    if positions is None:\n",
    "        positions = list(range(things['unstructured'].shape[1]))\n",
    "    \n",
    "    # Get indices of top k tokens (by maximum probability across all positions)\n",
    "    max_probs = np.maximum(\n",
    "        things['unstructured'].max(axis=1),\n",
    "        things['structured'].max(axis=1)\n",
    "    )\n",
    "    top_indices = np.argsort(max_probs)[-k:]\n",
    "    \n",
    "    # Create masked arrays for better visualization\n",
    "    def mask_array(arr):\n",
    "        if kind == \"logits\":\n",
    "            return np.ma.masked_where(arr < threshold, arr)\n",
    "        else:\n",
    "            return np.ma.masked_where(arr < threshold, arr)\n",
    "    \n",
    "    unstructured_masked = mask_array(things['unstructured'][top_indices][:, positions])\n",
    "    structured_masked = mask_array(things['structured'][top_indices][:, positions])\n",
    "\n",
    "    unstructured_masked, structured_masked = [(x - x.mean(0)) / x.std(0) for x in (unstructured_masked, structured_masked)]\n",
    "\n",
    "    # Create figure\n",
    "    if show_both:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8))\n",
    "        fig.suptitle(f'Token {kind.capitalize()} Evolution', fontsize=16, y=1.05)\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    \n",
    "    # Plot natural probabilities with masked array\n",
    "    im1 = ax1.imshow(\n",
    "        unstructured_masked,\n",
    "        aspect='auto',\n",
    "        cmap='viridis',\n",
    "    )\n",
    "    ax1.set_title(f'Natural Token {kind.capitalize()}')\n",
    "    ax1.set_xlabel('Position in Sequence')\n",
    "    ax1.set_ylabel('Token')\n",
    "    if show_tokens:\n",
    "        ax1.set_yticks(range(len(top_indices)))\n",
    "        ax1.set_yticklabels([vocab[i] for i in top_indices])\n",
    "    plt.colorbar(im1, ax=ax1, label=f'{kind.capitalize()}')\n",
    "    \n",
    "    # Plot constrained probabilities if requested\n",
    "    if show_both:\n",
    "        im2 = ax2.imshow(\n",
    "            structured_masked,\n",
    "            aspect='auto',\n",
    "            cmap='viridis',\n",
    "        )\n",
    "        ax2.set_title(f'Constrained Token {kind.capitalize()}')\n",
    "        ax2.set_xlabel('Position in Sequence')\n",
    "        ax2.set_yticks([])  # Hide y-ticks since they're the same as ax1\n",
    "        plt.colorbar(im2, ax=ax2, label=f'{kind.capitalize()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{prefix}{kind}_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7629f35f-0050-400d-8cb2-fd5074c4e9ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /HuggingFaceTB/SmolLM2-135M-Instruct/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 6ad97ada-bdd3-4bca-a7f1-109969e2e27e)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSSLCertVerificationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/util/ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/util/ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mSSLCertVerificationError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSSLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[31mSSLError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /HuggingFaceTB/SmolLM2-135M-Instruct/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSSLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moutlines\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43moutlines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFaceTB/SmolLM2-135M-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/outlines/models/transformers.py:430\u001b[39m, in \u001b[36mtransformers\u001b[39m\u001b[34m(model_name, device, model_kwargs, tokenizer_kwargs, model_class, tokenizer_class)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice_map\u001b[39m\u001b[33m\"\u001b[39m] = device\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m tokenizer_kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mpadding_side\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m tokenizer = tokenizer_class.from_pretrained(model_name, **tokenizer_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:487\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m    486\u001b[39m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m         resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m         commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/transformers/utils/hub.py:342\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m user_agent = http_user_agent(user_agent)\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    357\u001b[39m     resolved_file = _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/file_download.py:1071\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1067\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1069\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/file_download.py:1533\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1532\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1536\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1537\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1538\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/file_download.py:1450\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1447\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1450\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1461\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hf_raise_for_status(response)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:310\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/envs/jupyter-lab-3.12.8/lib/python3.12/site-packages/requests/adapters.py:698\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    694\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request=request)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    697\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mSSLError\u001b[39m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /HuggingFaceTB/SmolLM2-135M-Instruct/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 6ad97ada-bdd3-4bca-a7f1-109969e2e27e)')"
     ]
    }
   ],
   "source": [
    "import outlines\n",
    "\n",
    "model = outlines.models.transformers(\n",
    "    \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08382e-e4d8-4cd0-bcb4-ecf1c951c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee02ae8-c17c-4da2-9be2-ac050ace8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = outlines.generate.json(\n",
    "    model, \n",
    "    Person,\n",
    "    sampler = outlines.samplers.greedy()\n",
    ")\n",
    "\n",
    "# Add tools to track token probabilities as they are generated\n",
    "track_logits(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e2f8b-3094-4863-a0d2-4194be2f2a8c",
   "metadata": {},
   "source": [
    "## Chat Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaffe73-4e02-4845-aecf-8bf14b0e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(template(\n",
    "    model, \n",
    "    \"Give me a person with a name and an age.\",\n",
    "    system_prompt=\"You create users.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118d1e6-73e8-4053-92c4-a4beaf849d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any previously tracked logits\n",
    "generator.logits_processor.clear()\n",
    "\n",
    "person = generator(\n",
    "    template(\n",
    "        model, \n",
    "        \"Give me a person with a name and an age.\",\n",
    "        system_prompt=\"You create users.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c40251-e0ff-46fb-98a7-bfe04aa1e556",
   "metadata": {},
   "source": [
    "## Basic Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157268d-4392-4b64-8185-7c65ccbc2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(person.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b0e34-afb4-42c9-98cd-af52fd119ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_distributions(generator.logits_processor, positions=[0], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9d8a6-d62d-4a52-ad86-09cf2a298e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_distributions(generator.logits_processor, positions=[9,12], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5da7fa-f59d-40a2-be15-355a876a64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator.logits_processor.sequence(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285532c-4072-45d5-ae3d-a1d692f3449f",
   "metadata": {},
   "source": [
    "## You try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62bb74-17c1-48bc-8bbf-40d0f86874f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class EmployedPerson(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    job: Literal[\n",
    "        'Doctor', \n",
    "        'Basketball Player', \n",
    "        'Welder',\n",
    "        'Dog catcher'\n",
    "    ]\n",
    "\n",
    "gen = track_logits(\n",
    "    outlines.generate.json(\n",
    "        model, \n",
    "        EmployedPerson, \n",
    "        sampler=outlines.samplers.greedy()\n",
    "    )\n",
    ")\n",
    "\n",
    "person = gen(\n",
    "    template(\n",
    "        model, \n",
    "        \"Give me a person with a name, age, and job.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfecc6-1280-41dc-a9eb-78e25d4e37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_distributions(gen.logits_processor, positions=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768dbf8-a852-4025-9b55-70a0f983849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class Country(BaseModel):\n",
    "    capital: str\n",
    "    \n",
    "gen = track_logits(\n",
    "    outlines.generate.json(\n",
    "        model, \n",
    "        Country, \n",
    "        sampler=outlines.samplers.greedy()\n",
    "    )\n",
    ")\n",
    "\n",
    "person = gen(\n",
    "    template(\n",
    "        model, \n",
    "        \"What is the capital of France?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef490dd9-f26c-4a31-a986-6e0c57de6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_distributions(gen.logits_processor, positions=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66d6457-1258-4549-8b2f-a3d8e9db093d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCountry\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[32m      4\u001b[39m     capital: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m      6\u001b[39m gen = track_logits(\n\u001b[32m      7\u001b[39m     outlines.generate.json(\n\u001b[32m      8\u001b[39m         model, \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class Country(BaseModel):\n",
    "    capital: str\n",
    "    \n",
    "gen = track_logits(\n",
    "    outlines.generate.json(\n",
    "        model, \n",
    "        Country, \n",
    "        sampler=outlines.samplers.greedy()\n",
    "    )\n",
    ")\n",
    "\n",
    "person = gen(\n",
    "    template(\n",
    "        model, \n",
    "        \"Qual a capital do Brasil?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfab4a5-773b-448b-beae-ca20066ea2af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m tokens = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     token = get_token_distributions(\u001b[43mgen\u001b[49m.logits_processor, positions=[i], k=\u001b[32m5\u001b[39m, show_print=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     tokens.extend(token[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m      7\u001b[39m top_tokens = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(tokens)\n",
      "\u001b[31mNameError\u001b[39m: name 'gen' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for i in range(10):\n",
    "    token = get_token_distributions(gen.logits_processor, positions=[i], k=5, show_print=True)\n",
    "    tokens.extend(token[0][0])\n",
    "\n",
    "top_tokens = \"\".join(tokens)\n",
    "print(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bae055-c445-4bec-855c-3b2345d4043d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgen\u001b[49m.logits_processor.sequence(\u001b[32m30\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'gen' is not defined"
     ]
    }
   ],
   "source": [
    "print(gen.logits_processor.sequence(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb540c-da38-4c17-8115-fe68cfc97214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
